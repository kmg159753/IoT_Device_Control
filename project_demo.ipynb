{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "966d59c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82a46cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from model import model_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96726279",
   "metadata": {},
   "outputs": [],
   "source": [
    "facerec = dlib.face_recognition_model_v1('model_1113/dlib_face_recognition_resnet_model_v1.dat')\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "mp68=[143,116,123,147,192,210,211,208,199,428,431,430,416,376,352,345,372,\n",
    "70,63,105,66,107,\n",
    "336,296,334,293,300,\n",
    "168,197,5,4,75,97,2,326,305,\n",
    "33,160,158,133,153,144,362,385,387,263,373,\n",
    "380,61,39,37,0,267,269,291,405,314,17,84,181,78,82,13,312,308,317,14,87]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c56c4",
   "metadata": {},
   "source": [
    "# Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3187184",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def find_faces(image,results):\n",
    "    # Draw the face mesh annotations on the image.\n",
    "    rects=[]\n",
    "    shapes=dlib.full_object_detections()\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "# ------------------------------\n",
    "            skipface=False\n",
    "            points=dlib.points()\n",
    "            p=face_landmarks.landmark\n",
    "            ld68=[]\n",
    "            for i in mp68:\n",
    "                px=int(image.shape[1]*p[i].x)\n",
    "                py=int(image.shape[0]*p[i].y)\n",
    "                if px<0 or px>image.shape[1] or py<0 or py>image.shape[0]:\n",
    "                    skipface=True\n",
    "                    results.multi_face_landmarks.remove(face_landmarks)\n",
    "                    break\n",
    "                \n",
    "                ld68.append([px,py])\n",
    "                points.append(dlib.point(px,py))\n",
    "            \n",
    "            if skipface:\n",
    "                continue\n",
    "\n",
    "            cx=int((ld68[39][0]+ld68[42][0])/2)\n",
    "            cy=ld68[29][1]\n",
    "\n",
    "\n",
    "            x,y,w,h=cv2.boundingRect(np.array(ld68))\n",
    "            if w<h:\n",
    "                x-=int((h-w)/2)\n",
    "                w=h\n",
    "            elif w>h:\n",
    "                y-=int((w-h)/2)\n",
    "                h=w\n",
    "\n",
    "            rcx=int(x+(w/2))\n",
    "            rcy=int(y+(h/2))\n",
    "            x+=(cx-rcx)\n",
    "            y+=(cy-rcy)\n",
    "# ------------------------------\n",
    "    \n",
    "            rect=((x,y),(x+w,y+h))\n",
    "            rects.append(rect)\n",
    "            shape=dlib.full_object_detection(dlib.rectangle(x,y,x+w,y+h),points)\n",
    "            shapes.append(shape)\n",
    "    return rects,shapes,[]\n",
    "\n",
    "def rect_faces(image,results):\n",
    "    # Draw the face mesh annotations on the image.\n",
    "    rects=[]\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            \n",
    "            p=face_landmarks.landmark\n",
    "            ld68=[]\n",
    "            for i in mp68:\n",
    "                px=int(image.shape[1]*p[i].x)\n",
    "                py=int(image.shape[0]*p[i].y)\n",
    "                ld68.append([px,py])\n",
    "\n",
    "            cx=int((ld68[39][0]+ld68[42][0])/2)\n",
    "            cy=ld68[29][1]\n",
    "\n",
    "            x,y,w,h=cv2.boundingRect(np.array(ld68))\n",
    "            if w<h:\n",
    "                x-=int((h-w)/2)\n",
    "                w=h\n",
    "            elif w>h:\n",
    "                y-=int((w-h)/2)\n",
    "                h=w\n",
    "\n",
    "            rcx=int(x+(w/2))\n",
    "            rcy=int(y+(h/2))\n",
    "            x+=(cx-rcx)\n",
    "            y+=(cy-rcy)\n",
    "            \n",
    "            rect=((x,y),(x+w,y+h))\n",
    "            rects.append(rect)\n",
    "    return rects\n",
    "\n",
    "\n",
    "def encode_faces(img, shapes):\n",
    "    face_descriptors = facerec.compute_face_descriptor(img, shapes)\n",
    "\n",
    "    return np.array(face_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e54bc9fa",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def processFR(image,results,adminIndex):\n",
    "    # 얼굴 인식 후 사용자 face index 업데이트\n",
    "    # image - RGB\n",
    "    \n",
    "    rects, shapes, _ = find_faces(image,results)\n",
    "    descs = encode_faces(image, shapes)\n",
    "    admindiststemp={}\n",
    "\n",
    "    \n",
    "    for i, desc in enumerate(descs):\n",
    "    \n",
    "        found = False\n",
    "        names=[]\n",
    "        dists=[]\n",
    "        for name, admindesc in adminfacedescs.items():\n",
    "            dist = np.linalg.norm([desc] - admindesc, axis=1)\n",
    "            if dist < 0.5:\n",
    "                found = True\n",
    "                names.append(name)\n",
    "                dists.append(dist)\n",
    "\n",
    "        if found:\n",
    "            dists=np.array(dists)\n",
    "            idx=np.argmin(dists)\n",
    "            name=names[idx]\n",
    "            \n",
    "            if name in admindiststemp:\n",
    "                \n",
    "                if dists[idx]<admindiststemp[name][1]:\n",
    "                    admindiststemp[name]=[i,dists[idx]]\n",
    "            else:\n",
    "                admindiststemp[name]=[i,dists[idx]]\n",
    "            \n",
    "    # 중복 검출 필터링\n",
    "    for name, item in admindiststemp.items():\n",
    "        adminIndex[item[0]]=name\n",
    "        \n",
    "    return rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae0d7128",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T18:05:43.516858Z",
     "start_time": "2022-11-11T18:05:43.500250Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def processECD(image,rects,results,adminIndex):\n",
    "    global lastecd\n",
    "    global js\n",
    "    global activatedAdmin\n",
    "    global lastactivated\n",
    "    \n",
    "    face_imgs=None\n",
    "    face_order=[]\n",
    "    frame = Image.fromarray(image)\n",
    "    \n",
    "    if rects==-1:\n",
    "        rects=rect_faces(image,results)\n",
    "    if lastecd>=ecdterm:\n",
    "        # 응시 인식 진행\n",
    "        if activatedAdmin == '':\n",
    "            # 모든 사용자 응시 인식\n",
    "            for i,name in adminIndex.items():\n",
    "                l=rects[i][0][0]\n",
    "                t=rects[i][0][1]\n",
    "                r=rects[i][1][0]\n",
    "                b=rects[i][1][1]\n",
    "                # expand a bit\n",
    "                l -= (r-l)*0.2\n",
    "                r += (r-l)*0.2\n",
    "                t -= (b-t)*0.2\n",
    "                b += (b-t)*0.2\n",
    "                \n",
    "                face = frame.crop(([l,t,r,b]))\n",
    "                img = test_transforms(face)\n",
    "                img.unsqueeze_(0)\n",
    "                \n",
    "                if face_imgs is None:\n",
    "                    face_imgs=img\n",
    "                else:\n",
    "                    face_imgs = torch.cat([face_imgs, img])\n",
    "                \n",
    "                face_order.append(name)\n",
    "                \n",
    "        else:\n",
    "            # 특정 사용자 응시 인식\n",
    "            \n",
    "            for i,name in adminIndex.items():\n",
    "                if name==activatedAdmin:\n",
    "                    l=rects[i][0][0]\n",
    "                    t=rects[i][0][1]\n",
    "                    r=rects[i][1][0]\n",
    "                    b=rects[i][1][1]\n",
    "                    # expand a bit\n",
    "                    l -= (r-l)*0.2\n",
    "                    r += (r-l)*0.2\n",
    "                    t -= (b-t)*0.2\n",
    "                    b += (b-t)*0.2\n",
    "                    \n",
    "                    face = frame.crop(([l,t,r,b]))\n",
    "                    img = test_transforms(face)\n",
    "                    img.unsqueeze_(0)\n",
    "                    face_imgs=img\n",
    "                    \n",
    "                    face_order.append(name)\n",
    "                    break\n",
    "         \n",
    "        if face_imgs is not None:\n",
    "            # 모델 입력\n",
    "            output = model(face_imgs.cuda())\n",
    "            scores=torch.sigmoid(output)\n",
    "            idx=torch.argmax(output)\n",
    "            \n",
    "            # 최대값 인덱스\n",
    "            if activatedAdmin == '':\n",
    "                # 사용자 활성화\n",
    "                if scores[idx]>=0.85:\n",
    "                    activatedAdmin=face_order[idx]\n",
    "                    lastactivated=frametime\n",
    "                    js=1\n",
    "\n",
    "            else:\n",
    "                if scores[idx]<0.85:\n",
    "                    resetCheck()\n",
    "                else:\n",
    "                    lastactivated=frametime\n",
    "        lastecd=frametime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ee9a7a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T18:05:43.525692Z",
     "start_time": "2022-11-11T18:05:43.519788Z"
    }
   },
   "outputs": [],
   "source": [
    "def resetCheck():\n",
    "    global lastactivated\n",
    "    global activatedAdmin\n",
    "    global js\n",
    "    if frametime-lastactivated>=timeout:\n",
    "        activatedAdmin=''\n",
    "        js=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff986e",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ab92fca",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading saved model weights\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# load model weights\u001b[39;00m\n\u001b[0;32m      6\u001b[0m model_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_1113/model_weights.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_static\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m model_dict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[0;32m      9\u001b[0m snapshot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_weight)\n",
      "File \u001b[1;32m~\\model.py:11\u001b[0m, in \u001b[0;36mmodel_static\u001b[1;34m(pretrained, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading saved model weights\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m model_dict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m---> 11\u001b[0m snapshot \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m snapshot \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m snapshot\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m model_dict}\n\u001b[0;32m     13\u001b[0m model_dict\u001b[38;5;241m.\u001b[39mupdate(snapshot)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:713\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m    712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:930\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    928\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m    929\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m--> 930\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m deserialized_storage_keys \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m    934\u001b[0m offset \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;28;01mif\u001b[39;00m f_should_read_directly \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:876\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m    872\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_torch_load_uninitialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;66;03m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[0;32m    875\u001b[0m     deserialized_objects[root_key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39m_TypedStorage(\n\u001b[1;32m--> 876\u001b[0m         wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    877\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    879\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m deserialized_objects[root_key]\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m view_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 175\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:152\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 152\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    154\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:136\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    133\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    137\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    138\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    139\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    140\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    141\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "# set up data transformation\n",
    "test_transforms = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# load model weights\n",
    "model_weight='data_1113/model_weights.pkl'\n",
    "model = model_static(model_weight)\n",
    "model_dict = model.state_dict()\n",
    "snapshot = torch.load(model_weight)\n",
    "model_dict.update(snapshot)\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "model.cuda()\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f783ae7",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "adminList=['kim']\n",
    "\n",
    "adminfacedescs = {}\n",
    "\n",
    "activatedAdmin=''\n",
    "\n",
    "adminsEcdHistory={}\n",
    "\n",
    "ecdBuffer={}\n",
    "\n",
    "# System State\n",
    "ss=0\n",
    "# 얼굴 인식 진행\n",
    "# 모든 관리자 응시 인식 진행\n",
    "# 응시 중인 사용자 여부\n",
    "\n",
    "lastactivated=0\n",
    "\n",
    "# Jesture State\n",
    "js=0\n",
    "\n",
    "# ECD state\n",
    "es=0\n",
    "\n",
    "# last ecd time\n",
    "lastecd=0\n",
    "\n",
    "ecdterm=0.3\n",
    "\n",
    "timeout=2.0\n",
    "\n",
    "# Previous People\n",
    "pp=0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e9919fb",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# 사용자 얼굴 등록\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.5) as face_mesh:\n",
    "\n",
    "    img_bgr = cv2.imread('mingyu.jpg')\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(img_rgb)\n",
    "    rects, shapes, cords = find_faces(img_rgb,results)\n",
    "    adminfacedescs['kim']=encode_faces(img_rgb, shapes)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28676647",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b12c13a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T18:10:54.138988Z",
     "start_time": "2022-11-11T18:10:11.220536Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m img_h, img_w, img_c \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     25\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m results \u001b[38;5;241m=\u001b[39m face_mesh\u001b[38;5;241m.\u001b[39mprocess(image)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (results\u001b[38;5;241m.\u001b[39mmulti_face_landmarks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start Demo Program\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "cap = cv2.VideoCapture(0)\n",
    "pp=0\n",
    "frametime=time.time()\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=5,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.5) as face_mesh:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        start = time.time()\n",
    "        \n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Empty camera frame.\")\n",
    "            break\n",
    "            \n",
    "        \n",
    "        framegap=time.time()-frametime\n",
    "        frametime=time.time()\n",
    "        img_h, img_w, img_c = image.shape\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image)\n",
    "        \n",
    "        if (results.multi_face_landmarks is None):\n",
    "            pp=0\n",
    "            adminIndex={}\n",
    "            # 활성화 사용자 리셋 체크\n",
    "            if activatedAdmin!='':\n",
    "                resetCheck()\n",
    "        \n",
    "        elif (pp != len(results.multi_face_landmarks)):\n",
    "            # 얼굴 인식 진행\n",
    "            adminIndex={}\n",
    "            rects=processFR(image,results,adminIndex)\n",
    "            pp=len(results.multi_face_landmarks)\n",
    "            \n",
    "            # 응시 인식 진행\n",
    "            processECD(image,rects,results,adminIndex)\n",
    "        else:\n",
    "            rects=-1\n",
    "            # 응시 인식 진행\n",
    "            processECD(image,rects,results,adminIndex)\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        # Draw the face mesh annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        faceIdx=0\n",
    "        if results.multi_face_landmarks:\n",
    "              for face_landmarks in results.multi_face_landmarks:\n",
    "                    mediapipe_lm=np.array([np.multiply([p.x,p.y],[img_w,img_h]).astype(int) for p in face_landmarks.landmark])\n",
    "                    if faceIdx in adminIndex:\n",
    "                        cv2.putText(image, f'{adminIndex[faceIdx]}', mediapipe_lm[152], cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 20, 20), 2)\n",
    "                        \n",
    "                        \n",
    "                    faceIdx+=1\n",
    "        \n",
    "        end = time.time()\n",
    "        totalTime = end - start\n",
    "        \n",
    "        if activatedAdmin!='':\n",
    "            cv2.putText(image, f'{activatedAdmin}, Welcome.', (0,20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 20, 20), 2)\n",
    "            \n",
    "        cv2.putText(image, f'{totalTime}', (0,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (25, 20, 20), 2)\n",
    "        \n",
    "        cv2.imshow('MediaPipe Face Mesh', image)\n",
    "        wk=cv2.waitKey(1)\n",
    "        if wk & 0xFF == 27: # esc\n",
    "            break\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd22184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
